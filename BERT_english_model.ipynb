{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTZOIgciGOA7"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install editdistance\n",
        "\n",
        "import editdistance\n",
        "import pandas as pd\n",
        "import transformers\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.cuda\n",
        "import sklearn\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics import accuracy_score,f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from sklearn import preprocessing\n",
        "from collections import defaultdict\n",
        "from google.colab import drive\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "dataset = pd.read_csv('/content/drive/MyDrive/eng', sep=\"   \", header= None, error_bad_lines=False)\n",
        "\n",
        "dataset_copy=dataset.copy()\n",
        "list_data = dataset_copy.values.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split_data=[]\n",
        "\n",
        "def split(corpus, spit_data):\n",
        "  for sets in corpus:\n",
        "    for x in sets:\n",
        "      parts = x.split(\"\\t\")\n",
        "      spit_data.append(parts)\n",
        "\n",
        "split(list_data,split_data)\n",
        "\n",
        "print(list_data[0:100])\n",
        "print(split_data[0:100])"
      ],
      "metadata": {
        "id": "cu-rK-AQLiC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# removing duplicates\n",
        "def remove_duplicates(split_data):\n",
        "    unique_data = list(set(tuple(sublist) for sublist in split_data))\n",
        "    unique_data = [list(sublist) for sublist in unique_data]\n",
        "    return unique_data\n",
        "\n",
        "unique_data = remove_duplicates(split_data)\n",
        "print(unique_data[0:100])\n",
        "\n",
        "# getting the longest word for padding \n",
        "len_longest=0\n",
        "for string in unique_data:\n",
        "    x=string[1]\n",
        "    if len(x)>len_longest:\n",
        "      len_longest=len(x)"
      ],
      "metadata": {
        "id": "OCrv0BppSBqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting into words and grammar\n",
        "def split_word_and_grammar(unique_data_array):\n",
        "    last_elements = [subarray[-1] for subarray in unique_data_array]\n",
        "    word_list = [(subarray[0],subarray[1]) for subarray in unique_data_array]\n",
        "    return word_list, last_elements\n",
        "    \n",
        "word_list_eng, last_elements_eng = split_word_and_grammar(unique_data)\n",
        "\n",
        "print(word_list_eng[0:10])\n",
        "\n",
        "'''\n",
        "sep_token = \"SEP\"\n",
        "word_list_char_eng = []\n",
        "for tpl in word_list_eng:\n",
        "    new_word = []\n",
        "    for word in tpl:\n",
        "        new_word += [word] + [sep_token]\n",
        "    new_word.pop() # remove the extra separator token at the end\n",
        "    word_list_char_eng.append(new_word)\n",
        "'''\n",
        "\n",
        "# splitting grammar\n",
        "def splitting_grammar(last_elements):\n",
        "    list_of_lists = []\n",
        "    for info in last_elements:\n",
        "        parts = info.split(';')\n",
        "        num_parts = [int(part) if part.isdigit() else part for part in parts]\n",
        "        list_of_lists.append(num_parts)\n",
        "    return list_of_lists\n",
        "\n",
        "list_of_lists_mlt = splitting_grammar(last_elements_eng)"
      ],
      "metadata": {
        "id": "DbAyE02tgRZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#splitting data\n",
        "tokeniser = AutoTokenizer.from_pretrained('bert-base-cased', char_level=True) #,char_level=True\n",
        "model = AutoModelForSequenceClassification.from_pretrained('bert-base-cased')\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "X = word_list_mlt\n",
        "y = list_of_lists_mlt\n",
        "\n",
        "X_train, X_rem, y_train, y_rem = train_test_split(X,y, train_size=0.8)\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_rem,y_rem, test_size=0.2)\n",
        "\n",
        "print(y_train[0:10])\n",
        "print(len(X_train))"
      ],
      "metadata": {
        "id": "u5k7H1zAdvaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len_longest=len_longest+1\n",
        "\n",
        "token_indexes = tokeniser(X_train, return_tensors='pt', padding=True, truncation=True, is_split_into_words = False, max_length=512)\n",
        "indexed_train_x = token_indexes['input_ids'].to(device).squeeze()\n",
        "mask_train_x = token_indexes['attention_mask'].to(device).squeeze()\n",
        "\n",
        "new_tuples = [((\"\",) + t[1:]) for t in X_test]\n",
        "print(new_tuples[0:10])\n",
        "token_indexes_test = tokeniser(new_tuples, return_tensors = 'pt', padding = True, truncation = True, is_split_into_words = False, max_length = 512)\n",
        "indexed_test_x = token_indexes_test['input_ids'].to(device).squeeze()\n",
        "mask_test_x = token_indexes_test['attention_mask'].to(device).squeeze()\n",
        "\n",
        "# padding w/0 so gramm info is all the same length\n",
        "def padding_grammar(Y):\n",
        "    max_len = max(len(info) for info in Y)\n",
        "    padded_array = [info + ['<PAD>'] * (max_len - len(info)) for info in Y]\n",
        "    return padded_array\n",
        "padded_morph_info_train = padding_grammar(y_train)\n",
        "padded_morph_info_test = padding_grammar(y_test)\n",
        "\n",
        "# creating a dictionary for mapping \n",
        "morph_vocab = defaultdict(lambda: len(morph_vocab))\n",
        "for i in range(len(padded_morph_info_train[0])):\n",
        "    # extracting the unique values at this position\n",
        "    values = set(info[i] for info in padded_morph_info_train and padded_morph_info_test)\n",
        "    # assignign index\n",
        "    for value in values:\n",
        "        morph_vocab[value]\n",
        "print(morph_vocab)\n",
        "# map the morphological information to indexes using the vocabulary\n",
        "def get_targets_one_hot(padded_info):\n",
        "  targets = [[morph_vocab[value] for value in info] for info in padded_info]\n",
        "  targets = [[morph_vocab['<PAD>'] if value == 0 else value for value in info] for info in targets]\n",
        "  targets=torch.tensor(targets).to(device)\n",
        "\n",
        "  # one-hot encode the targets\n",
        "  num_classes = len(morph_vocab)\n",
        "  targets = torch.nn.functional.one_hot(targets, num_classes=num_classes).to(torch.float32)\n",
        "  targets = torch.tensor(targets, requires_grad=True).to(device)\n",
        "  return targets\n",
        "\n",
        "targets_train = get_targets_one_hot(padded_morph_info_train)\n",
        "targets_test = get_targets_one_hot(padded_morph_info_test)"
      ],
      "metadata": {
        "id": "4qqAAMoqgugA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(torch.nn.Module):\n",
        "    def __init__(self, bert):\n",
        "        super().__init__()\n",
        "        self.bert = bert\n",
        "        self.hidden_layer1 = torch.nn.Linear(768, 512)\n",
        "        self.batchnorm1 = torch.nn.BatchNorm1d(512)\n",
        "        self.drop1 = torch.nn.Dropout(p=0.5)\n",
        "        self.hidden_layer2 = torch.nn.Linear(512, 256)\n",
        "        self.batchnorm2 = torch.nn.BatchNorm1d(256)\n",
        "        self.drop2 = torch.nn.Dropout(p=0.2)\n",
        "        self.hidden_layer3 = torch.nn.Linear(256, 128)\n",
        "        self.batchnorm3 = torch.nn.BatchNorm1d(128)\n",
        "        self.drop3 = torch.nn.Dropout(p=0.2)\n",
        "        self.output_layer = torch.nn.Linear(128, 4*len(morph_vocab))\n",
        "    \n",
        "    def forward(self, x, mask):\n",
        "        with torch.cuda.amp.autocast():\n",
        "            vecs = self.bert(x, attention_mask=mask, output_hidden_states=True).hidden_states[8][:, 0, :]\n",
        "            hidden1 = self.hidden_layer1(vecs)\n",
        "            hidden1 = self.batchnorm1(hidden1)\n",
        "            hidden1 = torch.relu(hidden1)\n",
        "            hidden1 = self.drop1(hidden1)\n",
        "\n",
        "            hidden2 = self.hidden_layer2(hidden1)\n",
        "            hidden2 = self.batchnorm2(hidden2)\n",
        "            hidden2 = torch.relu(hidden2)\n",
        "            hidden2 = self.drop2(hidden2)\n",
        "\n",
        "            hidden3 = self.hidden_layer3(hidden2)\n",
        "            hidden3 = self.batchnorm3(hidden3)\n",
        "            hidden3 = torch.relu(hidden3)\n",
        "            hidden3 = self.drop3(hidden3)\n",
        "            \n",
        "            output = self.output_layer(hidden3).view(-1, 4, len(morph_vocab))\n",
        "            output = torch.sigmoid(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "bert = transformers.BertForMaskedLM.from_pretrained('bert-base-cased')\n",
        "model = Model(bert)\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "id": "yNnANdFhh_nG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_count=[]\n",
        "def get_acc(matches):\n",
        "    if len(matches) == 0:\n",
        "        return 0\n",
        "    accuracy_count.append(sum(matches)/len(matches))\n",
        "    return sum(matches)/len(matches)\n",
        "#return round(100*sum(preds)/len(preds),3)"
      ],
      "metadata": {
        "id": "y6vPbqd04_gv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.1) #, correct_bias=True\n",
        "\n",
        "print('step', 'error', 'accuracy')\n",
        "train_errors = []\n",
        "train_accs = []\n",
        "all_true=[]\n",
        "all_pred=[]\n",
        "accumulation_steps = 2 \n",
        "\n",
        "for step in range(1, 4000+1):\n",
        "    optimizer.zero_grad()\n",
        "    model.train(True)\n",
        "    loss_fn = torch.nn.CrossEntropyLoss()\n",
        "    all_outputs = []\n",
        "    all_targets = []\n",
        "    for i in range(accumulation_steps):\n",
        "        output = model(indexed_train_x[i*batch_size:(i+1)*batch_size], mask_train_x[i*batch_size:(i+1)*batch_size])\n",
        "       \n",
        "        error = loss_fn(output,targets_train[i*batch_size:(i+1)*batch_size])\n",
        "        error = error/accumulation_steps\n",
        "        error.backward()\n",
        "\n",
        "        all_outputs.append(torch.sigmoid(output).cpu().detach().numpy())\n",
        "        all_targets.append(targets_train[i*batch_size:(i+1)*batch_size].cpu().detach().numpy())\n",
        "\n",
        "    optimizer.step()\n",
        "    model.train(False)\n",
        "\n",
        "    train_errors.append(error.detach().tolist())\n",
        "\n",
        "    train_accs = []\n",
        "    \n",
        "  #  one_hot_output = np.round(output.cpu().detach()).numpy().astype(int)\n",
        "    \n",
        "    \n",
        "    for x in range(0,batch_size): # every vector in the output\n",
        "      targets = targets_train[i*batch_size:(i+1)*batch_size].cpu().detach()\n",
        "      targets = targets[x].argmax(dim=1).tolist()\n",
        "      predicted_labels = output.cpu().detach()[x].argmax(dim=1).tolist()\n",
        "      results=0\n",
        "\n",
        "      all_true.append(targets)\n",
        "      all_pred.append(predicted_labels)\n",
        "\n",
        "     # print('t',targets)\n",
        "     # print('p',predicted_labels)\n",
        "      for vecs1 in predicted_labels: # every vector in output vector i\n",
        "        for vecs2 in targets: # every vector in target vector i\n",
        "            if vecs1==vecs2:\n",
        "                results=results+1\n",
        "           \n",
        "      if results>=len(predicted_labels): #0.8*len(predicted_labels):\n",
        "        train_accs.append(1)\n",
        "      else:\n",
        "        train_accs.append(0)  \n",
        "    accuracy=get_acc(train_accs)\n",
        "\n",
        "    if step % 10 == 0:\n",
        "        print(step, train_errors[-1], accuracy)\n"
      ],
      "metadata": {
        "id": "dQTIkiB-AEkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = MultiLabelBinarizer().fit(all_true)\n",
        "\n",
        "f1_weighted=f1_score(m.transform(all_true),\n",
        "         m.transform(all_pred),\n",
        "         average='weighted')\n",
        "\n",
        "f1_macro=f1_score(m.transform(all_true),\n",
        "         m.transform(all_pred),\n",
        "         average='macro')\n",
        "\n",
        "print(\"F1 score weighted:\", f1_weighted)\n",
        "print(\"F1 score macro:\", f1_macro)\n",
        "\n",
        "\n",
        "all_true_flat = [label for sublist in all_true for label in sublist]\n",
        "all_pred_flat = [label for sublist in all_pred for label in sublist]\n",
        "\n",
        "f1_micro = f1_score(all_true_flat, all_pred_flat, average='weighted')\n",
        "print(\"F1 score micro:\", f1_micro)\n",
        "\n",
        "# problem with lev distance, uses order, order doesnt matter a lot here\n",
        "reverse_dict = {v: k for k, v in morph_vocab.items()} \n",
        "distances = []\n",
        "for i in range(len(all_true)):\n",
        "    predicted_labels = [reverse_dict[j] for j in all_pred[i]]\n",
        "    true_labels = [reverse_dict[j] for j in all_true[i]]\n",
        "    distance = editdistance.eval(predicted_labels, true_labels)\n",
        "    distances.append(distance)\n",
        "\n",
        "average_distance = sum(distances) / len(distances)\n",
        "print(\"Levenshtein distances:\", average_distance)\n",
        "\n",
        "avg= sum(accuracy_count)/len(accuracy_count)\n",
        "print(\"the average is: \", avg)"
      ],
      "metadata": {
        "id": "CjNERquSC66Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    print('sent', 'prediction')\n",
        "    for i in range(len(indexed_test_x) // batch_size):\n",
        "        outputs = model(indexed_test_x[i*batch_size:(i+1)*batch_size], mask_test_x[i*batch_size:(i+1)*batch_size])\n",
        "\n",
        "        # reversing the dict to get the grammar from the index\n",
        "        reverse_dict = {v: k for k, v in morph_vocab.items()} \n",
        "\n",
        "        for j, x in enumerate(outputs):\n",
        "            predicted_labels = [reverse_dict[i] for i in x.argmax(dim=1).tolist()]\n",
        "\n",
        "            # changing from char to word\n",
        "            input_word = ''.join(tokeniser.decode(indexed_test_x[i*batch_size:(i+1)*batch_size][j]).split())\n",
        "\n",
        "            print(input_word, predicted_labels)\n",
        "            print(\"actual\")\n",
        "            print(y_test[i*batch_size:(i+1)*batch_size][j])\n"
      ],
      "metadata": {
        "id": "r4JjnfjilXOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(fig, ax) = plt.subplots(1, 1)\n",
        "ax.set_xlabel('step')\n",
        "ax.set_ylabel('$E$')\n",
        "ax.plot(range(1, len(train_errors) + 1), train_errors, color='blue', linestyle='-', linewidth=3)\n",
        "ax.grid()\n",
        "\n",
        "(fig, ax) = plt.subplots(1, 1)\n",
        "ax.set_xlabel('step')\n",
        "ax.set_ylabel('$A$')\n",
        "ax.plot(range(1, len(accuracy_count) + 1), accuracy_count, color='blue', linestyle='-', linewidth=3)\n",
        "ax.grid()"
      ],
      "metadata": {
        "id": "S1WWbWYuAFE5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}