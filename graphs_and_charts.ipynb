{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qq0R_5vUVNkK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "dataset_mlt = pd.read_csv('/content/drive/MyDrive/Maltese_Unimorph_database_final.txt', sep=\"  \", header=None, error_bad_lines=False)\n",
        "\n",
        "dataset_copy_mlt=dataset_mlt.copy()\n",
        "list_data_mlt = dataset_copy_mlt.values.tolist()\n",
        "print(list_data_mlt[0:10])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split_data_mlt=[]\n",
        "\n",
        "def split(corpus, spit_data):\n",
        "  for sets in corpus:\n",
        "    for x in sets:\n",
        "      parts = x.split(\"\\t\")\n",
        "      spit_data.append(parts)\n",
        "\n",
        "split(list_data_mlt,split_data_mlt)\n",
        "print(split_data_mlt[0:100])\n",
        "\n",
        "print(len(split_data_mlt))"
      ],
      "metadata": {
        "id": "dyF2kH-VVSrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# removing duplicates\n",
        "def remove_duplicates(data_array):\n",
        "    unique_data_array = []\n",
        "    second_element_set = set()\n",
        "    for subarray in data_array:\n",
        "        if subarray[1] not in second_element_set:\n",
        "            unique_data_array.append(subarray)\n",
        "            second_element_set.add(subarray[1])\n",
        "    return unique_data_array\n",
        "\n",
        "unique_data_mlt = remove_duplicates(split_data_mlt)\n",
        "print(unique_data_mlt[0:100])\n",
        "\n",
        "#getting the longest word for padding \n",
        "len_longest=0\n",
        "for string in unique_data_mlt:\n",
        "    x=string[1]\n",
        "    if len(x)>len_longest:\n",
        "      len_longest=len(x)"
      ],
      "metadata": {
        "id": "DzEsdweSVUXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import pandas as pd\n",
        "\n",
        "# splitting into words and grammar\n",
        "def split_word_and_grammar(unique_data_array):\n",
        "    last_elements = [subarray[-1] for subarray in unique_data_array]\n",
        "    word_list = [(subarray[0],subarray[1]) for subarray in unique_data_array]\n",
        "    return word_list, last_elements\n",
        "\n",
        "word_list_mlt, last_elements_mlt = split_word_and_grammar(unique_data_mlt)\n",
        "print(last_elements_mlt[0:100])\n",
        "'''\n",
        "# converting to char level\n",
        "word_list_char_mlt = [[char for char in word] for word in word_list_mlt]\n",
        "word_list_char_ita = [[char for char in word] for word in word_list_ita]\n",
        "'''\n",
        "\n",
        "# splitting grammar\n",
        "def splitting_grammar(last_elements):\n",
        "    list_of_lists = []\n",
        "    for info in last_elements:\n",
        "        parts = info.split(';')\n",
        "        num_parts = [int(part) if part.isdigit() else part for part in parts]\n",
        "        list_of_lists.append(num_parts)\n",
        "    return list_of_lists\n",
        "\n",
        "list_of_lists_mlt = splitting_grammar(last_elements_mlt)\n",
        "\n",
        "\n",
        "# adding to lists in alt order\n",
        "split_data = []\n",
        "list_of_lists = []\n",
        "for i in range(len(word_list_mlt)):\n",
        "      # adding mlt word and grammar\n",
        "      array=(word_list_mlt[i][0],) + word_list_mlt[i][1:]\n",
        "      split_data.append(array)\n",
        "      list_of_lists.append(list_of_lists_mlt[i])\n",
        "\n",
        "print(split_data[0:100])\n",
        "print(list_of_lists[0:100])\n",
        "\n"
      ],
      "metadata": {
        "id": "JP9KuGs1VV8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.cuda"
      ],
      "metadata": {
        "id": "Hb8c8zytVXtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting data\n",
        "tokeniser = AutoTokenizer.from_pretrained(\"MLRS/BERTu\",char_level=True) #,char_level=True\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"MLRS/BERTu\")\n",
        "\n",
        "#tokeniser = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\",char_level=True)\n",
        "#model = AutoModelForMaskedLM.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "X = split_data\n",
        "y = list_of_lists\n",
        "\n",
        "X_train, X_rem, y_train, y_rem = train_test_split(X,y, train_size=0.8)\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_rem,y_rem, test_size=0.2)\n",
        "print(X_train[0:100])\n",
        "print(y_train[0:100])\n",
        "print(len(X_train))"
      ],
      "metadata": {
        "id": "OkaRkuzgVaCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "from sklearn import preprocessing\n",
        "from collections import defaultdict\n",
        "\n",
        "# tokenize the training set\n",
        "token_indexes = tokeniser(X_train, return_tensors='pt', padding=True, truncation=True, is_split_into_words=False, max_length=512)\n",
        "indexed_train_x = token_indexes['input_ids'].to(device).squeeze()\n",
        "mask_train_x = token_indexes['attention_mask'].to(device).squeeze()\n",
        "\n",
        "# tokenize the test set\n",
        "new_tuples = [((\"\",) + t[1:]) for t in X_test]\n",
        "token_indexes_test = tokeniser(new_tuples, return_tensors='pt', padding=True, truncation=True, is_split_into_words=False, max_length=512)\n",
        "indexed_test_x = token_indexes_test['input_ids'].to(device).squeeze()\n",
        "mask_test_x = token_indexes_test['attention_mask'].to(device).squeeze()\n",
        "\n",
        "# padding w/0 so gramm info is all the same length\n",
        "def padding_grammar(Y):\n",
        "    max_len = max(len(info) for info in Y)\n",
        "    padded_array = [info + ['<PAD>'] * (max_len - len(info)) for info in Y]\n",
        "    return padded_array\n",
        "padded_morph_info_train = padding_grammar(y_train)\n",
        "padded_morph_info_test = padding_grammar(y_test)\n",
        "print(padded_morph_info_train[0:10])\n",
        "\n",
        "all_morph_info = padded_morph_info_train + padded_morph_info_test\n",
        "# creating a dictionary for mapping \n",
        "morph_vocab = defaultdict(lambda: len(morph_vocab)) \n",
        "\n",
        "for i in range(len(all_morph_info[0])):\n",
        "    # extracting the unique values at this position\n",
        "    values = set(info[i] for info in all_morph_info)\n",
        "    # assignign index\n",
        "    for value in values:\n",
        "        morph_vocab[value]\n",
        "\n",
        "print(morph_vocab)\n"
      ],
      "metadata": {
        "id": "DU3W0CyTVbuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "all_inflections = []\n",
        "for key in morph_vocab:\n",
        "    all_inflections.append(key)\n",
        "print(all_inflections)\n",
        "\n",
        "# count the occurrences of each inflection in the list of lists\n",
        "inflection_counts = []\n",
        "for inflection in all_inflections:\n",
        "    count = 0\n",
        "    for inflection_list in y_train:\n",
        "        if inflection in inflection_list:\n",
        "            count += 1\n",
        "    for inflection_list in y_test:\n",
        "        if inflection in inflection_list:\n",
        "            count += 1\n",
        "    inflection_counts.append(count)\n",
        "\n",
        "print(inflection_counts)\n",
        "\n",
        "# calculate the frequencies of each inflection\n",
        "total_inflections = max(inflection_counts)\n",
        "inflection_freqs = [count for count in inflection_counts]\n",
        "print(total_inflections)\n",
        "\n",
        "# create a list of integers representing the positions of the bars\n",
        "x_values = range(len(all_inflections))\n",
        "\n",
        "# plot the counts as a bar chart\n",
        "fig, ax = plt.subplots()\n",
        "ax.bar(x_values, inflection_freqs)\n",
        "\n",
        "# set axis labels and title\n",
        "ax.set_xlabel('Inflections', fontsize=12)\n",
        "ax.set_ylabel('Counts', fontsize=12)\n",
        "ax.set_title('Inflection Counts', fontsize=14)\n",
        "\n",
        "# set the x-axis tick labels to the inflection names\n",
        "ax.set_xticks(x_values)\n",
        "ax.set_xticklabels(all_inflections, rotation=90, fontsize=8)\n",
        "\n",
        "# set the y-axis ticks\n",
        "max_count = max(inflection_counts)\n",
        "tick_step = max_count // 10 if max_count > 10 else 1\n",
        "ax.set_yticks(range(0, max_count + tick_step, tick_step))\n",
        "\n",
        "for tick in ax.yaxis.get_major_ticks():\n",
        "    tick.label.set_fontsize(8)\n",
        "    \n",
        "# show plot\n",
        "plt.show()\n",
        "'''\n",
        "import numpy as np\n",
        "\n",
        "# create a 2D array to store the confusion matrix\n",
        "confusion_matrix = np.zeros((len(list_of_lists), len(list_of_lists)))\n",
        "\n",
        "# loop over all pairs of lists and count the number of shared elements\n",
        "for i in range(len(list_of_lists)):\n",
        "    for j in range(i+1, len(list_of_lists)):\n",
        "        count = len(set(list_of_lists[i]).intersection(set(list_of_lists[j])))\n",
        "        confusion_matrix[i][j] = count\n",
        "        confusion_matrix[j][i] = count\n",
        "\n",
        "# print the confusion matrix\n",
        "print(confusion_matrix)\n",
        "'''"
      ],
      "metadata": {
        "id": "6CH0pQ0wVdj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "# create a ranking list of the inflections and the corresponding number of inflections\n",
        "ranked_inflections = sorted(zip(all_inflections, inflection_counts), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# create a list of lists containing the ranking, inflection, and count data\n",
        "table_data = []\n",
        "for i, (inflection, count) in enumerate(ranked_inflections):\n",
        "    table_data.append([i+1, inflection, count])\n",
        "\n",
        "# print the table\n",
        "print(tabulate(table_data, headers=[\"Rank\", \"Inflection\", \"Count\"]))\n",
        "\n"
      ],
      "metadata": {
        "id": "ymcysnNNVl-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# create a 2D array to store the confusion matrix\n",
        "confusion_matrix = np.zeros((len(list_of_lists), len(list_of_lists)))\n",
        "\n",
        "# loop over all pairs of lists and count the number of shared elements\n",
        "for i in range(len(list_of_lists)):\n",
        "    for j in range(i+1, len(list_of_lists)):\n",
        "        count = len(set(list_of_lists[i]).intersection(set(list_of_lists[j])))\n",
        "        confusion_matrix[i][j] = count\n",
        "        confusion_matrix[j][i] = count\n",
        "\n",
        "# print the confusion matrix\n",
        "print(confusion_matrix)\n"
      ],
      "metadata": {
        "id": "qyFYaXLoWEKA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}